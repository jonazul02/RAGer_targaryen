{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tutorial del uso de automodel for causal]('https://medium.com/@marketing_novita.ai/mastering-automodelforcausallm-a-handbook-for-novices-88ebdbad9736')\n",
    "\n",
    "[tutorial del uso de mistral como modelo](https://www.datacamp.com/tutorial/mistral-7b-tutorial)\n",
    "\n",
    "[access token huggingface](https://huggingface.co/settings/tokens)\n",
    "\n",
    "[memory usage](C:\\Users\\byacu\\.cache\\huggingface\\hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Proyectos\\Docker\\Multimodal_RAG\\amb_mm_rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "print(torch.cuda.is_available())  # Esto debe devolver True si tienes una GPU disponible\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el token desde las variables de entorno\n",
    "token = os.getenv(\"Mistral_access\")\n",
    "# Verifica que el token se haya cargado correctamente\n",
    "if not token:\n",
    "    raise ValueError(\"No se pudo cargar el token MISTRAL_ACCESS_TOKEN. Aseg√∫rate de que est√© configurado correctamente en el archivo .env.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# Cargar el tokenizador y el modelo Mistral 7B\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si el tokenizador tiene un pad_token, si no, asignar el eos_token como pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar si la GPU est√° disponible y mover el modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    # Tokenizar el input y mover los datos a la GPU\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generar respuesta\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], max_length=150, do_sample=True)\n",
    "\n",
    "    # Decodificar la respuesta\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    # Tokenizar el input, agregando padding y attention_mask\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generar respuesta\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],  # A√±adir attention mask\n",
    "            max_length=150,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id  # Asegurarnos de que se utiliza el pad_token_id\n",
    "        )\n",
    "\n",
    "    # Decodificar la respuesta\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido al chat con Mistral 7B. Escribe algo para empezar (escribe 'salir' para terminar):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral: Hi, how are you? Yesterday I was in another country and I‚Äôm now here. This was one of the 3 biggest surprises of last night, the other 2 also have to do with a country. üòÄ\n",
      "\n",
      "I had an excellent, relaxed and extremely productive weekend. And to finish it perfectly, I could watch a movie I really like, so I got on Netflix in the morning and started searching for ‚ÄúThe Grand Budapest Hotel‚Äù.\n",
      "\n",
      "When it finally appeared on my screen, I pressed play and‚Ä¶ oh my goodness, I was in Austria! The weather is the same as the one from ‚ÄúThe Grand Budapest Hotel‚Äù. üòÄ\n",
      "\n",
      "Now I know that I‚Äôll only be here 3\n",
      "\n",
      "CPU times: total: 35 s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Bienvenido al chat con Mistral 7B. Escribe algo para empezar (escribe 'salir' para terminar):\")\n",
    "user_input = input(\"T√∫: \")\n",
    "if user_input.lower() == \"salir\":\n",
    "    print(\"¬°Adi√≥s!\")\n",
    "\n",
    "\n",
    "# Generar y mostrar la respuesta del modelo\n",
    "response = generate_response(user_input)\n",
    "print(f\"Mistral: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilGPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Proyectos\\Docker\\Multimodal_RAG\\amb_mm_rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\byacu\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar si la GPU est√° disponible y mover el modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    # Tokenizar el input y moverlo a la GPU\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Medir el tiempo de inferencia\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generar la respuesta\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=150, \n",
    "            do_sample=True, \n",
    "            pad_token_id=tokenizer.eos_token_id  # Evitar advertencias\n",
    "        )\n",
    "\n",
    "    # Medir el tiempo final\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Decodificar la respuesta\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido al chat con DistilGPT-2. Escribe algo para empezar (escribe 'salir' para terminar):\n",
      "DistilGPT-2: Que haces? What were the worst fears on the face of the world. What did they end up doing?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‚ÄúThe first three months of June are the worst yet, according to this report by the UN and US Congress‚Äô . In their response to the ongoing refugee crisis we‚Äôre dealing with the humanitarian crisis.\n",
      "‚ÄúThe UN is demanding a resolution that is consistent with the UN Refugee Convention which was signed in the wake of the refugee crisis, and is calling for a resumption of the process.\"\n",
      "‚ÄúThe United Nations estimates that every 1 million asylum seeker resettled would be internally displaced or those who are forced to leave their country.\n",
      "‚ÄúThere are many ways of doing\n",
      "Tiempo de inferencia: 2.8726 segundos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Bienvenido al chat con DistilGPT-2. Escribe algo para empezar (escribe 'salir' para terminar):\")\n",
    "\n",
    "\n",
    "user_input = input(\"T√∫: \")\n",
    "if user_input.lower() == \"salir\":\n",
    "    print(\"¬°Adi√≥s!\")\n",
    "\n",
    "# Generar y mostrar la respuesta del modelo\n",
    "response, time_taken = generate_response(user_input)\n",
    "print(f\"DistilGPT-2: {response}\")\n",
    "print(f\"Tiempo de inferencia: {time_taken:.4f} segundos\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-fine tunned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Proyectos\\Docker\\Multimodal_RAG\\amb_mm_rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\byacu\\.cache\\huggingface\\hub\\models--datificate--gpt2-small-spanish. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Cargar el tokenizador y el modelo GPT-2 ajustado para espa√±ol\n",
    "model_name = \"datificate/gpt2-small-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar si la GPU est√° disponible y mover el modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], max_length=150, do_sample=True)\n",
    "    \n",
    "    response_time = time.time() - start_time\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response, response_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido al chat en espa√±ol. Escribe algo para empezar (escribe 'salir' para terminar):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 (Espa√±ol): ¬øCu√°les son las mejores prestaciones paraempleados que debes dar en tu PyME? En tu sistema de correo electr√≥nico hay que pedir al tu PC que tengate y que pague una m√°quina a tu PC.\n",
      "\n",
      "Existen modelos de PC a tama√±o entero. Algunos se encuentran en los √∫ltimos a√±os.\n",
      "\n",
      "La √∫ltima serie de PC fue hecha para usuarios de PC y por ello han nacido los PC's de hasta para ser m√°s grandes para ellos. Existen algunas variantes y PC's de hasta 10 MB.\n",
      "\n",
      "Algunos de los t√≠tulos que m√°s resaltan son: \n",
      "\n",
      "\n",
      "En esta serie los t√≠tulos en 2 partes fueron lanzados al mercado con PC el a√±o 2004. El primero de este fue el mismo PC llamado K-21 (\n",
      "Tiempo de inferencia: 3.7642 segundos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Bienvenido al chat en espa√±ol. Escribe algo para empezar (escribe 'salir' para terminar):\")\n",
    "\n",
    "user_input = input(\"T√∫: \")\n",
    "if user_input.lower() == \"salir\":\n",
    "    print(\"¬°Adi√≥s!\")\n",
    "\n",
    "response, time_taken = generate_response(user_input)\n",
    "print(f\"GPT-2 (Espa√±ol): {response}\")\n",
    "print(f\"Tiempo de inferencia: {time_taken:.4f} segundos\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amb_mm_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
